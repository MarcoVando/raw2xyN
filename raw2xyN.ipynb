{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pJgP20zcjlIP",
    "outputId": "c44519b0-42a9-422c-aef0-6463e23001ae"
   },
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8XNWp0CDsWz6",
    "outputId": "cf62d3dc-dcdc-420a-e7d0-3a5bdf82d02a"
   },
   "outputs": [],
   "source": [
    "!pip install swig\n",
    "!pip install Boost\n",
    "!pip install xylib-py\n",
    "# !git clone https://github.com/wojdyr/xylib.git\n",
    "# !pip install git+https://github.com/wojdyr/xylib.git\n",
    "# !python setup.py install\n",
    "\n",
    "!pip install swig Boost \n",
    "export PATH=$PATH:\n",
    "!pip install xylib-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j55K3z0uko5E"
   },
   "outputs": [],
   "source": [
    "import xylib as xy\n",
    "\n",
    "import glob, os \n",
    "\n",
    "#data management\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhlO1iH9oCE8"
   },
   "source": [
    "xylib is a library for reading files that contain x-y data from powder diffraction, spectroscopy or other experimental methods.\n",
    "It is recommended to set LC_NUMERIC=\"C\" (or other locale with the same numeric format) before reading files.  Usually, we first call load_file() to read file from disk. It stores all data from the file in class DataSet.\n",
    "  DataSet contains a list of Blocks, each Blocks contains a list of Columns,\n",
    "  and each Column contains a list of values.\n",
    " \n",
    "  It may sound complex, but IMO it can't be made simpler.\n",
    "  It's analogical to a spreadsheet. One OOCalc or Excel file (which\n",
    "  corresponds to xylib::DataSet) contains a number of sheets (Blocks),\n",
    "  but usually only one is used. Each sheet can be viewed as a list of columns.\n",
    " \n",
    "  In xylib all columns in one block must have equal length.\n",
    "  Several filetypes always contain only one Block with two Columns.\n",
    "  In this case we can take coordinates of the 15th point as:\n",
    "     double x = get_block(0)->get_column(1)->get_value(14);\n",
    "     double y = get_block(0)->get_column(2)->get_value(14);\n",
    "  Note that blocks and points are numbered from 0, but columns are numbered\n",
    "  from 1, because the column 0 returns index of point.\n",
    "  All values are stored as floating-point numbers, even if they are integers\n",
    "  in the file.\n",
    "  DataSet and Block contain also MetaData, which is a string to string map.\n",
    " \n",
    "  Note that C++ API uses std::string and exceptions, so it is recommended\n",
    "  to compile the library and programs that use it with the same compiler.\n",
    " \n",
    "  C++ API is defined in xylib namespace, C API use prefix xylib.\n",
    " /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65N-BJVQfppD"
   },
   "source": [
    "Read file and get block and the two columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wrzEJtsik0mM",
    "outputId": "82cb2cff-f93a-45ff-9224-35bcf99f46d3"
   },
   "outputs": [],
   "source": [
    "filename = 'FILENAME.raw'\n",
    "file1= xy.load_file('./' + filename)\n",
    "\n",
    "print(file1.get_block_count())\n",
    "block = file1.get_block(0)\n",
    "print(block.get_column_count())\n",
    "col1 = block.get_column(1)\n",
    "col2 = block.get_column(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZAzP_D-fDwl"
   },
   "source": [
    "Get metadata from which u can obtain the x axis (seems that col1 is not accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NiXjRFaiVHx3",
    "outputId": "6827001a-1024-424a-92e9-3659c59ec9d1"
   },
   "outputs": [],
   "source": [
    "print(block.get_name())\n",
    "meta = block.meta\n",
    "type(meta)\n",
    "\n",
    "keys = []\n",
    "\n",
    "#get the keys name\n",
    "for i in range(0,12):\n",
    "  keys.append(meta.get_key(i))\n",
    "\n",
    "#loop over keys to get their values\n",
    "for key in keys:\n",
    "  print(key ,meta.get(key))\n",
    "startth = meta.get('START_2THETA')\n",
    "stepsize = meta.get('STEP_SIZE')\n",
    "nstep = meta.get('STEPS')\n",
    "\n",
    "print(type(startth), type(stepsize), type(nstep))\n",
    "print(float(startth), float(stepsize), float(nstep))\n",
    "\n",
    "startth = float(startth)\n",
    "stepsize = float(stepsize)\n",
    "nstep = float(nstep)\n",
    "\n",
    "# creat np array for x\n",
    "npx = np.arange(startth,startth+stepsize*nstep,stepsize)\n",
    "np.shape(npx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67aeHxj2gfPR"
   },
   "source": [
    "Extract y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JAzXAtDr9Lp7",
    "outputId": "192505cb-ee89-47b0-b6a8-3ea335134589"
   },
   "outputs": [],
   "source": [
    "valy = []\n",
    "\n",
    "for index in range(1, col2.get_point_count()):\n",
    "  valy.append(col2.get_value(index))\n",
    "\n",
    "# create numpy array\n",
    "npy = np.array(valy)\n",
    "\n",
    "#build the dataset\n",
    "dataset = [npx[:-1], npy]\n",
    "\n",
    "npdataset = np.array(dataset).T\n",
    "np.shape(npdataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Vf5PBKDhmRF"
   },
   "source": [
    "construct the head of the final file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4dF_GxxRhqQL"
   },
   "outputs": [],
   "source": [
    "tps = meta.get('TIME_PER_STEP')\n",
    "lam = meta.get('USED_LAMBDA')\n",
    "head = f\"time per step: {tps}\\nlambda: {lam}\\n2theta Counts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "tTmQ5zRRtim4",
    "outputId": "52583e13-c673-4b78-a1bd-8a602f5c4a5b"
   },
   "outputs": [],
   "source": [
    "# plot dataset\n",
    "plt.plot(dataset[0], dataset[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAlVgf9iovrw"
   },
   "source": [
    "Export and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0nTWJaItu63K"
   },
   "outputs": [],
   "source": [
    "np.savetxt(fname=filename[:-3]+\".xy\", X=npdataset, header=head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdQsTy0xovRD"
   },
   "source": [
    "The following cell is to run the same code on multiple files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3UGCCkjQou9B",
    "outputId": "7a124f42-9cad-451f-d871-ac0f4be898ff"
   },
   "outputs": [],
   "source": [
    "    import xylib as xy\n",
    "\n",
    "    mainDir = path\n",
    "    subDirs = []\n",
    "\n",
    "    selected = []    #REMEMBER to put extension. \n",
    "\n",
    "    #file parameter\n",
    "    ext = '.raw'\n",
    "\n",
    "    #variable declaration\n",
    "    paths = []\n",
    "    files = []  #is a list of filenames choosen\n",
    "    dataset = []  #is a list of np.array rapresenting the whole dataset\n",
    "\n",
    "    #Generate the complete paths for each defined subdir\n",
    "    if len(subDirs) > 1:\n",
    "        for subDir in subDirs:\n",
    "            paths.append(mainDir+subDir)\n",
    "    else:\n",
    "        paths.append(mainDir)\n",
    "    if debug:\n",
    "        print(\"looking in:\\n\", paths)\n",
    "\n",
    "    #import files in selected \n",
    "    if len(selected) == 0:\n",
    "        if len(subDirs) != 0:\n",
    "            selected = glob.glob(mainDir+'**/*'+ext, recursive=True)   # if filenames is empty, take all folders/files inside maindir\n",
    "        else:\n",
    "            selected = glob.glob(mainDir+'*'+ext, recursive=True)\n",
    "\n",
    "    #sort selected files\n",
    "    selected.sort()\n",
    "    if debug:\n",
    "        print(f'sorting the {len(selected)} selected files by the name:', selected)\n",
    "    \n",
    "    #look for already converted files. delete those from selected list\n",
    "    selectedToRemove = []   #container for the indexes to be removed\n",
    "    for i in range(0,len(selected)):\n",
    "        if os.path.basename(selected[i])[:-3]+'xy' in os.listdir(path):\n",
    "            selectedToRemove.append(i)\n",
    "    if len(selectedToRemove) != 0:\n",
    "        if debug:\n",
    "            print(f\"found {len(selectedToRemove)} already converted files\")\n",
    "        for index in range(1, len(selectedToRemove)+1):\n",
    "            selected.pop(len(selectedToRemove)-index)  #pop items on reverse so that indexes won't change during the popping procedure.\n",
    "        selected.sort()\n",
    "        if debug:\n",
    "            print(f'sorting the {len(selected)} updated selected files by the name:', selected)\n",
    "    \n",
    "            \n",
    "    #start the conversion\n",
    "    for pathFile in selected:\n",
    "        if debug:\n",
    "            print(f'converting {os.path.basename(pathFile)}\\n')\n",
    "        try:\n",
    "            file = xy.load_file(pathFile)\n",
    "        except RuntimeError:\n",
    "            print(f\"!!! raw format not supported for {os.path.basename(pathFile)}\")\n",
    "            formatErrors +=1\n",
    "            \n",
    "        \n",
    "        #usually raws are made of 1 block\n",
    "        block = file.get_block(0)\n",
    "\n",
    "        #extract the two columns, even though seems that col1 doesn't contain 2theta values\n",
    "        col1 = block.get_column(1)\n",
    "        col2 = block.get_column(2)\n",
    "        if debug:\n",
    "            print(f\"file: {file}, n blocks: {file.get_block_count()}, n cols in block: {block.get_column_count()}\")\n",
    "\n",
    "        #extract meta information from which we can build-up 2theta axis.\n",
    "        meta = block.meta\n",
    "        keys = []   #container for keys\n",
    "\n",
    "        #get the keys' name\n",
    "        for i in range(0,12):\n",
    "            keys.append(meta.get_key(i))\n",
    "\n",
    "        #loop over keys to get their values\n",
    "        for key in keys:\n",
    "            print(key ,meta.get(key))\n",
    "        startth = float(meta.get('START_2THETA'))\n",
    "        stepsize = float(meta.get('STEP_SIZE'))\n",
    "        nstep = float(meta.get('STEPS'))\n",
    "        tps = float(meta.get('TIME_PER_STEP'))\n",
    "        lam = float(meta.get('USED_LAMBDA'))\n",
    "\n",
    "        # create np array for x\n",
    "        npx = np.arange(startth,startth+stepsize*nstep,stepsize)\n",
    "        if debug:\n",
    "            print(f\"{np.shape(npx)} points on x axis\" )\n",
    "            print(f\"x values loaded from {npx[0]} to {npx[-1]} with stepsize {npx[1]-npx[0]}\")\n",
    "\n",
    "        # extract y values\n",
    "        valy = []\n",
    "        for index in range(0, col2.get_point_count()):\n",
    "            valy.append(col2.get_value(index))\n",
    "\n",
    "        # create numpy array\n",
    "        npy = np.array(valy)\n",
    "        if debug:\n",
    "            print(f\"{np.shape(npy)} points on y axis\" )\n",
    "        npnorm = npy/tps\n",
    "\n",
    "        #build the dataset\n",
    "        if np.shape(npy) != np.shape(npx):\n",
    "            dataset = [npx[:-1], npy, npnorm]   #usually x points are 1 time longer.\n",
    "        else:\n",
    "            dataset = [npx, npy, npnorm]\n",
    "\n",
    "        #generate np.array\n",
    "        npdataset = np.array(dataset).T\n",
    "\n",
    "        #build the head of the final .xy files\n",
    "        head = f\"time per step: {tps}\\nlambda: {lam}\\n2theta Counts NormalizedCounts\"\n",
    "    \n",
    "        \n",
    "        if expInSubDir == True:\n",
    "            if not os.path.exists(path+'/export'):\n",
    "                !mkdir {'\\''+pathFile.strip(os.path.basename(pathFile))+'export/'+'\\''}\n",
    "            fnameXY = path+'export/'+os.path.basename(pathFile)[0:-4]+'.xy'\n",
    "            fnameXYN = path+'export/'+os.path.basename(pathFile)[0:-4]+'.xyn'\n",
    "        else:\n",
    "            fnameXY = path+os.path.basename(pathFile)[0:-4]+'.xy'\n",
    "            fnameXYN = path+os.path.basename(pathFile)[0:-4]+'.xyn'\n",
    "            \n",
    "        # save array to .xy file\n",
    "        np.savetxt(fname=fnameXY, X=npdataset[:,:2], header=head)\n",
    "        np.savetxt(fname=fnameXYN, X=npdataset[:,:2], header=head)\n",
    "\n",
    "    print('###########################################################################\\n')\n",
    "    print(f'Procedure terminated with {formatErrors} errors due to non-supported files')\n",
    "else:\n",
    "    print('###########################################################################\\n')\n",
    "    print('No .raw files are found')\n",
    "        #!zip -r export.zip export"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
